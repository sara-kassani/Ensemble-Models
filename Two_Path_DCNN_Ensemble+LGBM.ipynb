{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score\n",
    "from keras.layers import Dense, Input, Dropout, Activation, Conv2D, MaxPooling2D, Lambda, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Flatten, Input, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import he_normal, glorot_normal\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.applications import DenseNet201\n",
    "from keras.applications import DenseNet121\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train/'\n",
    "test_dir = 'data/test/'\n",
    "\n",
    "extracted_features_dir = \"extracted_features/\"\n",
    "model_name = \"VGG16_DenseNet201-LGBM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train/ 0\n",
      "data/train/BCC 100\n",
      "data/train/BKL 100\n",
      "data/train/AKIEC 100\n",
      "******************************\n",
      "data/test/ 0\n",
      "data/test/BCC 10\n",
      "data/test/BKL 10\n",
      "data/test/AKIEC 10\n"
     ]
    }
   ],
   "source": [
    "for root,dirs,files in os.walk(train_dir):\n",
    "    print (root, len(files))\n",
    "\n",
    "print(\"*\"*30)\n",
    "for root,dirs,files in os.walk(test_dir):\n",
    "    print (root, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height, img_width = 224, 224\n",
    "input_shape = (img_height, img_width, 3)\n",
    "epochs = 10\n",
    "\n",
    "top_model_path = os.path.join(extracted_features_dir, 'model_'+model_name+'_model.h5')\n",
    "top_model_weights_path = os.path.join(extracted_features_dir, 'model_'+model_name+'_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 225 images belonging to 3 classes.\n",
      "Found 75 images belonging to 3 classes.\n",
      "Found 30 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "random_seed = np.random.seed(1142)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    validation_split= 0.25,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    subset = 'training',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    subset = 'validation',\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_train_samples: 225\n",
      "nb_validation_samples: 75\n",
      "nb_test_samples: 30\n",
      "\n",
      "predict_size_train: 8\n",
      "predict_size_validation: 3\n",
      "predict_size_test: 1\n",
      "\n",
      " num_classes: 3\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\n",
    "predict_size_test = int(math.ceil(nb_test_samples / batch_size))\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "print(\"nb_train_samples:\", nb_train_samples)\n",
    "print(\"nb_validation_samples:\", nb_validation_samples)\n",
    "print(\"nb_test_samples:\", nb_test_samples)\n",
    "\n",
    "print(\"\\npredict_size_train:\", predict_size_train)\n",
    "print(\"predict_size_validation:\", predict_size_validation)\n",
    "print(\"predict_size_test:\", predict_size_test)\n",
    "\n",
    "print(\"\\n num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel1=VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "x1=basemodel1.get_layer('block5_pool').output\n",
    "x1=GlobalAveragePooling2D()(x1)\n",
    "\n",
    "basemodel2=DenseNet201(weights=None,input_tensor = basemodel1.input, include_top=False, input_shape=input_shape)\n",
    "x2 = basemodel2.output\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "\n",
    "merge = concatenate([x1, x2])\n",
    "merge = Dropout(0.6)(merge)\n",
    "preds = Dense(num_classes, activation='softmax')(merge)\n",
    "bottleneck_final_model = Model(inputs=basemodel1.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = bottleneck_final_model.predict_generator(train_generator, predict_size_train, max_q_size=1, pickle_safe=False)\n",
    "np.save(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy', bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_validation = bottleneck_final_model.predict_generator(validation_generator, predict_size_validation)\n",
    "np.save(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy', bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_test = bottleneck_final_model.predict_generator(test_generator, predict_size_test)\n",
    "np.save(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy', bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def reset_keras_tf_session():\n",
    "    \"\"\"\n",
    "    this function clears the gpu memory and set the \n",
    "    tf session to not use the whole gpu\n",
    "    \"\"\"\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "reset_keras_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy')\n",
    "validation_data = np.load(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy')\n",
    "test_data = np.load(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy')\n",
    "\n",
    "train_labels = train_generator.classes\n",
    "validation_labels = validation_generator.classes\n",
    "test_labels = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape : (225, 3)\n",
      "Training Data label Shape : (225,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Shape : {0}\".format(train_data.shape))\n",
    "print(\"Training Data label Shape : {0}\".format(train_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightgbmClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.70833333 0.54166667 0.41666667 0.29166667 0.42857143\n",
      " 0.23809524 0.42857143 0.47619048 0.33333333]\n",
      "accuracy mean and std 0.44 +/- 0.13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.50      0.70      0.58        10\n",
      "           2       0.25      0.30      0.27        10\n",
      "\n",
      "   micro avg       0.33      0.33      0.33        30\n",
      "   macro avg       0.25      0.33      0.29        30\n",
      "weighted avg       0.25      0.33      0.29        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lgbm=lgb.LGBMClassifier(n_estimators=1000, class_weight=\"balanced\", reg_alpha=0.1, reg_lambda=0.1, learning_rate=0.001, num_leaves=400,\n",
    "                        random_state=523, boosting='dart')\n",
    "lgbm_scores=cross_val_score(lgbm,train_data, train_labels, cv=10)\n",
    "print(lgbm_scores)\n",
    "print(\"accuracy mean and std %.2f\" %np.mean(lgbm_scores),\"+/- %.2f\"%np.std(lgbm_scores))\n",
    "\n",
    "lgbm.fit(train_data, train_labels)\n",
    "y_pred=lgbm.predict(test_data)\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "0.26666666666666666\n",
      "0.4\n",
      "Random Forest test accuracies 0.4000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_data, train_labels)\n",
    "print(clf.score(train_data, train_labels))\n",
    "print(clf.score(validation_data, validation_labels ))\n",
    "print(clf.score(test_data, test_labels))\n",
    "\n",
    "y_test_pred = clf.predict(test_data)\n",
    "clf_test = accuracy_score(test_labels, y_test_pred)\n",
    "print('Random Forest test accuracies %.4f' % (clf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.30      0.40        10\n",
      "           1       0.42      0.50      0.45        10\n",
      "           2       0.31      0.40      0.35        10\n",
      "\n",
      "   micro avg       0.40      0.40      0.40        30\n",
      "   macro avg       0.44      0.40      0.40        30\n",
      "weighted avg       0.44      0.40      0.40        30\n",
      "\n",
      "[[3 3 4]\n",
      " [0 5 5]\n",
      " [2 4 4]]\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, y_test_pred))\n",
    "print(confusion_matrix(test_labels, y_test_pred))\n",
    "print(accuracy_score(test_labels, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ################## SVM ###################################\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(C=1.0, cache_size=500, class_weight=None, coef0=0.0,\n",
    "              decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
    "              max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "clf.fit(pred_trn, y_trn)\n",
    "print(clf.score(pred_vld, y_vld))\n",
    "\n",
    "# %%############## F1 Score #################################\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print(f1_score(y_vld, pred, average='weighted'))\n",
    "conf_matrix = confusion_matrix(y_vld, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
